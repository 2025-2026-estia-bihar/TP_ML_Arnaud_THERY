{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd8e246",
   "metadata": {},
   "source": [
    "## 1. Imports et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50ca4fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è Device: cuda\n",
      "   GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "   CUDA Version: 13.0\n",
      "\n",
      "‚úì Imports et configuration termin√©s\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchsummary import summary\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# LIME for interpretability\n",
    "import lime\n",
    "from lime import lime_image\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# DEVICE CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"   Running on CPU\")\n",
    "\n",
    "# Random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "print(\"\\n‚úì Imports et configuration termin√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cb340d",
   "metadata": {},
   "source": [
    "## 2. Acquisition des Donn√©es\n",
    "\n",
    "### Instructions de t√©l√©chargement\n",
    "\n",
    "**Dataset Kaggle:** https://www.kaggle.com/datasets/rodrigonuneswessner/labeledcorndataset\n",
    "\n",
    "**Option 1 - Kaggle API:**\n",
    "```bash\n",
    "# Installer kaggle CLI\n",
    "pip install kaggle\n",
    "\n",
    "# Configurer API token (~/.kaggle/kaggle.json)\n",
    "kaggle datasets download -d rodrigonuneswessner/labeledcorndataset\n",
    "unzip labeledcorndataset.zip -d ../data/corn_images/\n",
    "```\n",
    "\n",
    "**Option 2 - Manuel:**\n",
    "1. T√©l√©charger depuis Kaggle\n",
    "2. D√©zipper dans `../data/corn_images/`\n",
    "\n",
    "**Structure attendue:**\n",
    "```\n",
    "data/corn_images/\n",
    "‚îú‚îÄ‚îÄ chao/          # ground (sol)\n",
    "‚îú‚îÄ‚îÄ milho/         # corn (ma√Øs)\n",
    "‚îú‚îÄ‚îÄ hervas/        # weeds (mauvaises herbes)\n",
    "‚îî‚îÄ‚îÄ milho_ervas/   # corn/weeds (mixte)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d768d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONFIGURATION DU PROJET\n",
      "======================================================================\n",
      "Data root: ../data/corn_images/ImagensTCCRotuladas/ImagensTCCRotuladas\n",
      "Train directory: ../data/corn_images/ImagensTCCRotuladas/ImagensTCCRotuladas/Treino\n",
      "Test directory: ../data/corn_images/ImagensTCCRotuladas/ImagensTCCRotuladas/Teste\n",
      "Validation directory: ../data/corn_images/ImagensTCCRotuladas/ImagensTCCRotuladas/Valida‚îú–∑‚îú–≥o\n",
      "Image size: (224, 224)\n",
      "Batch size: 32\n",
      "Epochs: 30\n",
      "\n",
      "3 classes: ['ground', 'corn', 'weeds']\n",
      "4 classes: ['ground', 'corn', 'weeds', 'corn_weeds']\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION DES CHEMINS\n",
    "# =============================================================================\n",
    "\n",
    "# Le dataset est d√©j√† divis√© en Train/Test/Validation\n",
    "DATA_ROOT = Path(\"../data/corn_images/ImagensTCCRotuladas/ImagensTCCRotuladas\")\n",
    "TRAIN_DIR = DATA_ROOT / \"Treino\"\n",
    "TEST_DIR = DATA_ROOT / \"Teste\"\n",
    "# Utiliser glob pour g√©rer l'encodage du nom Valida√ß√£o\n",
    "VAL_DIR = list(DATA_ROOT.glob(\"Valida*\"))[0]\n",
    "\n",
    "# Mapping labels portugais ‚Üí anglais\n",
    "CLASS_MAPPING = {\n",
    "    'Chao': 'ground',\n",
    "    'Milho': 'corn', \n",
    "    'Ervas': 'weeds',\n",
    "    'Milho_ervas': 'corn_weeds'\n",
    "}\n",
    "\n",
    "# Configuration exp√©rimentation\n",
    "CONFIG = {\n",
    "    # Phase 1: 3 classes (ground, corn, weeds)\n",
    "    'classes_3': ['Chao', 'Milho', 'Ervas'],\n",
    "    \n",
    "    # Phase 2: 4 classes (ajout corn/weeds)\n",
    "    'classes_4': ['Chao', 'Milho', 'Ervas', 'Milho_ervas'],\n",
    "    \n",
    "    # Hyperparam√®tres\n",
    "    'img_size': (224, 224),  # VGG16/ResNet standard\n",
    "    'batch_size': 32,\n",
    "    'epochs': 30,\n",
    "    'validation_split': 0.2,\n",
    "    'test_split': 0.1,\n",
    "    \n",
    "    # Optimisation\n",
    "    'learning_rate': 0.001,\n",
    "    'dropout_rate': 0.5,\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURATION DU PROJET\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"Train directory: {TRAIN_DIR}\")\n",
    "print(f\"Test directory: {TEST_DIR}\")\n",
    "print(f\"Validation directory: {VAL_DIR}\")\n",
    "print(f\"Image size: {CONFIG['img_size']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Epochs: {CONFIG['epochs']}\")\n",
    "print(f\"\\n3 classes: {[CLASS_MAPPING[c] for c in CONFIG['classes_3']]}\")\n",
    "print(f\"4 classes: {[CLASS_MAPPING[c] for c in CONFIG['classes_4']]}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33c947e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä TRAIN Dataset Statistics (3 classes):\n",
      "----------------------------------------------------------------------\n",
      "ground          (Chao        ):  6134 images\n",
      "corn            (Milho       ):  6255 images\n",
      "weeds           (Ervas       ):  6015 images\n",
      "Total                        : 18404 images\n",
      "\n",
      "üìä TEST Dataset Statistics (3 classes):\n",
      "----------------------------------------------------------------------\n",
      "ground          (Chao        ):   100 images\n",
      "corn            (Milho       ):   100 images\n",
      "weeds           (Ervas       ):   100 images\n",
      "Total                        :   300 images\n",
      "\n",
      "üìä VALIDATION Dataset Statistics (3 classes):\n",
      "----------------------------------------------------------------------\n",
      "ground          (Chao        ):   646 images\n",
      "corn            (Milho       ):   695 images\n",
      "weeds           (Ervas       ):   668 images\n",
      "Total                        :  2009 images\n",
      "\n",
      "üìä TRAIN Dataset Statistics (4 classes):\n",
      "----------------------------------------------------------------------\n",
      "ground          (Chao        ):  6134 images\n",
      "corn            (Milho       ):  6255 images\n",
      "weeds           (Ervas       ):  6015 images\n",
      "corn_weeds      (Milho_ervas ):  6040 images\n",
      "Total                        : 24444 images\n",
      "\n",
      "‚öñÔ∏è Class Imbalance Ratio (3 classes - TRAIN): 1.04\n",
      "   ‚Üí Classes relativement √©quilibr√©es\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# V√âRIFICATION DONN√âES & STRUCTURE\n",
    "# =============================================================================\n",
    "\n",
    "def check_data_availability(data_dir: Path, classes: List[str]) -> Dict:\n",
    "    \"\"\"V√©rifie la pr√©sence et compte les images par classe.\"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    if not data_dir.exists():\n",
    "        print(f\"‚ùå ERROR: {data_dir} does not exist!\")\n",
    "        return stats\n",
    "    \n",
    "    for class_name in classes:\n",
    "        class_dir = data_dir / class_name\n",
    "        if class_dir.exists():\n",
    "            images = list(class_dir.glob(\"*.jpg\")) + list(class_dir.glob(\"*.png\"))\n",
    "            stats[class_name] = len(images)\n",
    "        else:\n",
    "            stats[class_name] = 0\n",
    "            print(f\"‚ö† WARNING: {class_dir} not found\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Check TRAIN set (3 classes)\n",
    "print(\"\\nüìä TRAIN Dataset Statistics (3 classes):\")\n",
    "print(\"-\" * 70)\n",
    "stats_train_3 = check_data_availability(TRAIN_DIR, CONFIG['classes_3'])\n",
    "for class_name, count in stats_train_3.items():\n",
    "    print(f\"{CLASS_MAPPING[class_name]:15s} ({class_name:12s}): {count:5d} images\")\n",
    "print(f\"{'Total':15s} {' '*13}: {sum(stats_train_3.values()):5d} images\")\n",
    "\n",
    "# Check TEST set (3 classes)\n",
    "print(\"\\nüìä TEST Dataset Statistics (3 classes):\")\n",
    "print(\"-\" * 70)\n",
    "stats_test_3 = check_data_availability(TEST_DIR, CONFIG['classes_3'])\n",
    "for class_name, count in stats_test_3.items():\n",
    "    print(f\"{CLASS_MAPPING[class_name]:15s} ({class_name:12s}): {count:5d} images\")\n",
    "print(f\"{'Total':15s} {' '*13}: {sum(stats_test_3.values()):5d} images\")\n",
    "\n",
    "# Check VALIDATION set (3 classes)\n",
    "print(\"\\nüìä VALIDATION Dataset Statistics (3 classes):\")\n",
    "print(\"-\" * 70)\n",
    "stats_val_3 = check_data_availability(VAL_DIR, CONFIG['classes_3'])\n",
    "for class_name, count in stats_val_3.items():\n",
    "    print(f\"{CLASS_MAPPING[class_name]:15s} ({class_name:12s}): {count:5d} images\")\n",
    "print(f\"{'Total':15s} {' '*13}: {sum(stats_val_3.values()):5d} images\")\n",
    "\n",
    "# Check 4 classes (TRAIN only for overview)\n",
    "print(\"\\nüìä TRAIN Dataset Statistics (4 classes):\")\n",
    "print(\"-\" * 70)\n",
    "stats_train_4 = check_data_availability(TRAIN_DIR, CONFIG['classes_4'])\n",
    "for class_name, count in stats_train_4.items():\n",
    "    print(f\"{CLASS_MAPPING[class_name]:15s} ({class_name:12s}): {count:5d} images\")\n",
    "print(f\"{'Total':15s} {' '*13}: {sum(stats_train_4.values()):5d} images\")\n",
    "\n",
    "# Balance check\n",
    "if stats_train_3:\n",
    "    max_count = max(stats_train_3.values())\n",
    "    min_count = min(stats_train_3.values())\n",
    "    imbalance_ratio = max_count / min_count if min_count > 0 else 0\n",
    "    print(f\"\\n‚öñÔ∏è Class Imbalance Ratio (3 classes - TRAIN): {imbalance_ratio:.2f}\")\n",
    "    if imbalance_ratio > 2:\n",
    "        print(\"   ‚Üí D√©s√©quilibre significatif: augmentation de donn√©es recommand√©e\")\n",
    "    else:\n",
    "        print(\"   ‚Üí Classes relativement √©quilibr√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2cea99",
   "metadata": {},
   "source": [
    "## 3. Analyse Exploratoire des Donn√©es (EDA)\n",
    "\n",
    "Cette section explore:\n",
    "- Distribution des classes\n",
    "- Tailles d'images (width, height)\n",
    "- Statistiques RGB (moyenne, √©cart-type)\n",
    "- Exemples visuels par classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "701162e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stats_3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m     plt.show()\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Visualisation 3 classes\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mstats_3\u001b[49m:\n\u001b[32m     35\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müñºÔ∏è Visualisation des √©chantillons (3 classes):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     36\u001b[39m     plot_samples(BASE_DIR, CONFIG[\u001b[33m'\u001b[39m\u001b[33mclasses_3\u001b[39m\u001b[33m'\u001b[39m], n_samples=\u001b[32m5\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'stats_3' is not defined"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# VISUALISATION √âCHANTILLONS PAR CLASSE\n",
    "# =============================================================================\n",
    "\n",
    "def plot_samples(base_dir: Path, classes: List[str], n_samples: int = 5, figsize=(15, 10)):\n",
    "    \"\"\"Affiche n √©chantillons al√©atoires de chaque classe.\"\"\"\n",
    "    fig, axes = plt.subplots(len(classes), n_samples, figsize=figsize)\n",
    "    \n",
    "    for i, class_name in enumerate(classes):\n",
    "        class_dir = base_dir / class_name\n",
    "        images = list(class_dir.glob(\"*.jpg\")) + list(class_dir.glob(\"*.png\"))\n",
    "        \n",
    "        if len(images) == 0:\n",
    "            continue\n",
    "            \n",
    "        # S√©lection al√©atoire\n",
    "        samples = np.random.choice(images, min(n_samples, len(images)), replace=False)\n",
    "        \n",
    "        for j, img_path in enumerate(samples):\n",
    "            img = load_img(img_path)\n",
    "            ax = axes[i, j] if len(classes) > 1 else axes[j]\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "            \n",
    "            if j == 0:\n",
    "                ax.set_ylabel(f\"{CLASS_MAPPING[class_name]}\\n({class_name})\", \n",
    "                            fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('√âchantillons par Classe', fontsize=14, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualisation 3 classes\n",
    "if stats_3:\n",
    "    print(\"\\nüñºÔ∏è Visualisation des √©chantillons (3 classes):\")\n",
    "    plot_samples(BASE_DIR, CONFIG['classes_3'], n_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d713d34",
   "metadata": {},
   "source": [
    "**Observations visuelles:**\n",
    "- `ground` (chao): Sol sec, couleurs terre/beige, pas de v√©g√©tation\n",
    "- `corn` (milho): Feuilles vertes de ma√Øs, texture v√©g√©tale homog√®ne\n",
    "- `weeds` (hervas): Plantes herbac√©es diverses, feuilles plus petites/d√©sordonn√©es\n",
    "- `corn/weeds` (milho_ervas): M√©lange visible des deux types de v√©g√©tation\n",
    "\n",
    "**Difficult√©s attendues:**\n",
    "1. Distinction `corn` vs `corn/weeds`: pr√©sence partielle difficile √† d√©tecter\n",
    "2. Variabilit√© d'√©clairage (photos smartphone en ext√©rieur)\n",
    "3. Angles de vue vari√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65e0ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìê Analyse des dimensions d'images (√©chantillon 100/classe):\n",
      "\n",
      "Statistiques globales:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['width', 'height', 'aspect_ratio'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Statistiques\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStatistiques globales:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf_sizes\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwidth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mheight\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43maspect_ratio\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m.describe())\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Visualisation\u001b[39;00m\n\u001b[32m     39\u001b[39m fig, axes = plt.subplots(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, figsize=(\u001b[32m14\u001b[39m, \u001b[32m5\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Aflokkat/TP_ML/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4119\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4118\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4119\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4121\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Aflokkat/TP_ML/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Aflokkat/TP_ML/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:6261\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6260\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6261\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index(['width', 'height', 'aspect_ratio'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ANALYSE DISTRIBUTION DES TAILLES D'IMAGES\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_image_sizes(base_dir: Path, classes: List[str], n_sample: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"Analyse les dimensions des images (√©chantillon al√©atoire).\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for class_name in classes:\n",
    "        class_dir = base_dir / class_name\n",
    "        images = list(class_dir.glob(\"*.jpg\")) + list(class_dir.glob(\"*.png\"))\n",
    "        \n",
    "        # √âchantillonnage pour acc√©l√©rer\n",
    "        sampled = np.random.choice(images, min(n_sample, len(images)), replace=False)\n",
    "        \n",
    "        for img_path in sampled:\n",
    "            img = load_img(img_path)\n",
    "            width, height = img.size\n",
    "            data.append({\n",
    "                'class': class_name,\n",
    "                'class_label': CLASS_MAPPING[class_name],\n",
    "                'width': width,\n",
    "                'height': height,\n",
    "                'aspect_ratio': width / height,\n",
    "                'total_pixels': width * height\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "if stats_3:\n",
    "    print(\"\\nüìê Analyse des dimensions d'images (√©chantillon 100/classe):\")\n",
    "    df_sizes = analyze_image_sizes(BASE_DIR, CONFIG['classes_3'], n_sample=100)\n",
    "    \n",
    "    # Statistiques\n",
    "    print(\"\\nStatistiques globales:\")\n",
    "    print(df_sizes[['width', 'height', 'aspect_ratio']].describe())\n",
    "    \n",
    "    # Visualisation\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Distribution width/height\n",
    "    axes[0].scatter(df_sizes['width'], df_sizes['height'], \n",
    "                   c=pd.Categorical(df_sizes['class_label']).codes, \n",
    "                   alpha=0.6, cmap='viridis')\n",
    "    axes[0].set_xlabel('Width (px)')\n",
    "    axes[0].set_ylabel('Height (px)')\n",
    "    axes[0].set_title('Distribution des dimensions')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Aspect ratio\n",
    "    df_sizes.boxplot(column='aspect_ratio', by='class_label', ax=axes[1])\n",
    "    axes[1].set_title('Aspect Ratio par classe')\n",
    "    axes[1].set_xlabel('Classe')\n",
    "    axes[1].set_ylabel('Aspect Ratio (width/height)')\n",
    "    \n",
    "    plt.suptitle('Analyse G√©om√©trique des Images', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úì Redimensionnement √† {CONFIG['img_size']} n√©cessaire pour uniformit√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda29780",
   "metadata": {},
   "source": [
    "## 4. Pr√©traitement et Augmentation de Donn√©es\n",
    "\n",
    "### Strat√©gies de pr√©traitement:\n",
    "1. **Redimensionnement**: 224√ó224 (standard VGG16/ResNet)\n",
    "2. **Normalisation**: [0, 255] ‚Üí [0, 1] (rescale=1./255)\n",
    "3. **Augmentation** (train uniquement):\n",
    "   - Rotation: ¬±20¬∞\n",
    "   - Zoom: ¬±15%\n",
    "   - Flip horizontal\n",
    "   - Shift: ¬±10% (width/height)\n",
    "\n",
    "**Justification:** Les photos sont prises en conditions naturelles avec variabilit√© d'angle et d'√©clairage ‚Üí augmentation robustifie le mod√®le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acb10ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA GENERATORS (avec augmentation pour training)\n",
    "# =============================================================================\n",
    "\n",
    "def create_data_generators(base_dir: Path, \n",
    "                          classes: List[str],\n",
    "                          img_size: Tuple[int, int],\n",
    "                          batch_size: int,\n",
    "                          validation_split: float = 0.2):\n",
    "    \"\"\"\n",
    "    Cr√©e les g√©n√©rateurs train/validation avec augmentation.\n",
    "    \n",
    "    Returns:\n",
    "        train_gen, val_gen, class_indices\n",
    "    \"\"\"\n",
    "    \n",
    "    # G√©n√©rateur TRAIN avec augmentation\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,              # Normalisation [0,1]\n",
    "        rotation_range=20,           # Rotation ¬±20¬∞\n",
    "        width_shift_range=0.1,       # Shift horizontal ¬±10%\n",
    "        height_shift_range=0.1,      # Shift vertical ¬±10%\n",
    "        zoom_range=0.15,             # Zoom ¬±15%\n",
    "        horizontal_flip=True,        # Flip al√©atoire\n",
    "        fill_mode='nearest',         # Remplissage pixels manquants\n",
    "        validation_split=validation_split\n",
    "    )\n",
    "    \n",
    "    # G√©n√©rateur VALIDATION (pas d'augmentation)\n",
    "    val_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        validation_split=validation_split\n",
    "    )\n",
    "    \n",
    "    # Flow from directory (train)\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        base_dir,\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        classes=classes,\n",
    "        subset='training',\n",
    "        shuffle=True,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # Flow from directory (validation)\n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        base_dir,\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        classes=classes,\n",
    "        subset='validation',\n",
    "        shuffle=False,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    return train_generator, val_generator, train_generator.class_indices\n",
    "\n",
    "# Cr√©ation des g√©n√©rateurs (3 classes)\n",
    "if stats_3:\n",
    "    print(\"\\nüîÑ Cr√©ation des g√©n√©rateurs de donn√©es (3 classes):\")\n",
    "    print(\"=\"*70)\n",
    "    train_gen_3, val_gen_3, class_indices_3 = create_data_generators(\n",
    "        BASE_DIR,\n",
    "        CONFIG['classes_3'],\n",
    "        CONFIG['img_size'],\n",
    "        CONFIG['batch_size'],\n",
    "        CONFIG['validation_split']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nClass indices: {class_indices_3}\")\n",
    "    print(f\"Train samples: {train_gen_3.samples}\")\n",
    "    print(f\"Validation samples: {val_gen_3.samples}\")\n",
    "    print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "    print(f\"Steps per epoch (train): {train_gen_3.samples // CONFIG['batch_size']}\")\n",
    "    print(f\"Validation steps: {val_gen_3.samples // CONFIG['batch_size']}\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1497e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALISATION AUGMENTATION\n",
    "# =============================================================================\n",
    "\n",
    "def visualize_augmentation(generator, n_images: int = 4):\n",
    "    \"\"\"Affiche des exemples d'images augment√©es.\"\"\"\n",
    "    batch = next(generator)\n",
    "    images = batch[0][:n_images]\n",
    "    labels = batch[1][:n_images]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_images, figsize=(15, 4))\n",
    "    \n",
    "    # Reverse class indices for display\n",
    "    idx_to_class = {v: CLASS_MAPPING[k] for k, v in generator.class_indices.items()}\n",
    "    \n",
    "    for i in range(n_images):\n",
    "        axes[i].imshow(images[i])\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "        # Get predicted class\n",
    "        class_idx = np.argmax(labels[i])\n",
    "        class_name = idx_to_class[class_idx]\n",
    "        axes[i].set_title(f\"{class_name}\", fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('Exemples d\\'images augment√©es (training set)', \n",
    "                fontsize=13, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if stats_3:\n",
    "    print(\"\\nüé® Visualisation de l'augmentation de donn√©es:\")\n",
    "    visualize_augmentation(train_gen_3, n_images=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7437b314",
   "metadata": {},
   "source": [
    "## 5. Mod√©lisation - Phase 1: Baseline CNN (3 classes)\n",
    "\n",
    "### Architecture CNN Simple\n",
    "\n",
    "**Justification du design:**\n",
    "- 3 blocs Conv2D + MaxPooling (extraction features hi√©rarchiques)\n",
    "- BatchNormalization apr√®s chaque conv (stabilit√© training)\n",
    "- Dropout 0.5 avant classification (r√©gularisation)\n",
    "- Dense layer finale softmax (3 classes)\n",
    "\n",
    "**Optimiseur:** Adam (lr=0.001) - adaptatif, converge rapidement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd1ff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CNN BASELINE (3 classes)\n",
    "# =============================================================================\n",
    "\n",
    "def build_baseline_cnn(input_shape: Tuple[int, int, int], \n",
    "                      num_classes: int,\n",
    "                      dropout_rate: float = 0.5) -> keras.Model:\n",
    "    \"\"\"\n",
    "    Construit un CNN simple baseline.\n",
    "    \n",
    "    Architecture:\n",
    "        Conv2D(32) ‚Üí BatchNorm ‚Üí MaxPool ‚Üí Dropout(0.25)\n",
    "        Conv2D(64) ‚Üí BatchNorm ‚Üí MaxPool ‚Üí Dropout(0.25)\n",
    "        Conv2D(128) ‚Üí BatchNorm ‚Üí MaxPool ‚Üí Dropout(0.25)\n",
    "        Flatten ‚Üí Dense(256) ‚Üí Dropout(0.5) ‚Üí Dense(num_classes)\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Block 1\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', \n",
    "                     input_shape=input_shape, name='conv1'),\n",
    "        layers.BatchNormalization(name='bn1'),\n",
    "        layers.MaxPooling2D((2, 2), name='pool1'),\n",
    "        layers.Dropout(0.25, name='dropout1'),\n",
    "        \n",
    "        # Block 2\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='conv2'),\n",
    "        layers.BatchNormalization(name='bn2'),\n",
    "        layers.MaxPooling2D((2, 2), name='pool2'),\n",
    "        layers.Dropout(0.25, name='dropout2'),\n",
    "        \n",
    "        # Block 3\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='conv3'),\n",
    "        layers.BatchNormalization(name='bn3'),\n",
    "        layers.MaxPooling2D((2, 2), name='pool3'),\n",
    "        layers.Dropout(0.25, name='dropout3'),\n",
    "        \n",
    "        # Classifier\n",
    "        layers.Flatten(name='flatten'),\n",
    "        layers.Dense(256, activation='relu', name='fc1'),\n",
    "        layers.Dropout(dropout_rate, name='dropout_fc'),\n",
    "        layers.Dense(num_classes, activation='softmax', name='predictions')\n",
    "    ], name='baseline_cnn')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Construction du mod√®le\n",
    "if stats_3:\n",
    "    print(\"\\nüèóÔ∏è Construction du CNN Baseline (3 classes):\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    input_shape = (*CONFIG['img_size'], 3)\n",
    "    baseline_cnn_3 = build_baseline_cnn(input_shape, num_classes=3, \n",
    "                                       dropout_rate=CONFIG['dropout_rate'])\n",
    "    \n",
    "    # Compilation\n",
    "    baseline_cnn_3.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=CONFIG['learning_rate']),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    baseline_cnn_3.summary()\n",
    "    \n",
    "    print(\"\\n‚úì Mod√®le compil√© avec:\")\n",
    "    print(f\"  - Optimiseur: Adam (lr={CONFIG['learning_rate']})\")\n",
    "    print(f\"  - Loss: categorical_crossentropy\")\n",
    "    print(f\"  - Dropout: {CONFIG['dropout_rate']}\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac6c4e4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**√Ä CONTINUER:**\n",
    "\n",
    "Les prochaines cellules √† impl√©menter:\n",
    "1. ‚úÖ Training du baseline CNN (3 classes) avec callbacks\n",
    "2. ‚úÖ √âvaluation + courbes accuracy/loss\n",
    "3. ‚úÖ Matrice de confusion\n",
    "4. ‚úÖ Transfer Learning (VGG16, ResNet50)\n",
    "5. ‚úÖ Extension 4 classes\n",
    "6. ‚úÖ Interpr√©tation avec LIME\n",
    "\n",
    "**Note:** Le notebook sera ex√©cut√© apr√®s t√©l√©chargement du dataset. Pour l'instant, structure et m√©thodologie valid√©es ‚úì"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
